{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning: Artificial Neural Networks\n",
    "\n",
    "Instructions\n",
    "_____________________________\n",
    "\n",
    "This file contains code that helps you get started. \n",
    "You will need to complete the following functions \n",
    " \n",
    "    - predict.m\n",
    "    - sigmoidGradient.m\n",
    "    - randInitializeWeights.m\n",
    "    - nnCostFunction.m\n",
    "\n",
    "For this exercise, you will not need to change any code in this file, or any other files other than those mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (nnCostFunction.py, line 75)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[0;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3378\u001b[0m in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[1;36m  Cell \u001b[1;32mIn [1], line 8\u001b[1;36m\n\u001b[1;33m    from nnCostFunction import nnCostFunction\u001b[1;36m\n",
      "\u001b[1;36m  File \u001b[1;32m~\\Desktop\\UM_bachelor\\year_2\\Machine Learning\\Lab 5\\Jupyter\\Initial code\\nnCostFunction.py:75\u001b[1;36m\u001b[0m\n\u001b[1;33m    for t in range(0,5000):\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "\n",
    "from predict import predict\n",
    "from displayData import displayData\n",
    "from sigmoidGradient import sigmoidGradient\n",
    "from randInitializeWeights import randInitializeWeights\n",
    "from nnCostFunction import nnCostFunction\n",
    "from checkNNGradients import checkNNGradients\n",
    "from fmincg import fmincg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the parameters you will use for this exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_size = 400;     # 20x20 Input Images of Digits\n",
    "hidden_layer_size = 25;     # 25 hidden units\n",
    "num_labels = 10;            # 10 labels, from 0 to 9   \n",
    "                            # (note that we have mapped \"0\" to label 9 to follow\n",
    "                            # the same structure used in the MatLab version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =========== Part 1: Loading and Visualizing Data =============\n",
    "We start the exercise by first loading and visualizing the dataset. \n",
    "You will be working with a dataset that contains handwritten digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading and Visualizing Data ...')\n",
    "\n",
    "mat = scipy.io.loadmat('C:/Users/sophi/Desktop/UM_bachelor/year_2/Machine Learning/Lab 5/Jupyter/Initial code/digitdata.mat')\n",
    "X = mat['X']\n",
    "y = mat['y']\n",
    "y = np.squeeze(y)\n",
    "m, _ = np.shape(X)\n",
    "\n",
    "# Randomly select 100 data points to display\n",
    "sel = np.random.choice(range(X.shape[0]), 100)\n",
    "sel = X[sel,:]\n",
    "\n",
    "displayData(sel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ================ Part 2: Loading Pameters ================\n",
    "In this part of the exercise, we load some pre-initialized neural network parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading Saved Neural Network Parameters ...')\n",
    "\n",
    "# Load the weights into variables Theta1 and Theta2\n",
    "mat = scipy.io.loadmat('debugweights.mat');\n",
    "\n",
    "# Unroll parameters\n",
    "Theta1 = mat['Theta1']\n",
    "Theta1_1d = np.reshape(Theta1, Theta1.size, order='F')\n",
    "Theta2 = mat['Theta2']\n",
    "Theta2_1d = np.reshape(Theta2, Theta2.size, order='F')\n",
    "\n",
    "nn_params = np.hstack((Theta1_1d, Theta2_1d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ================= Part 3: Implement Predict =================\n",
    "After training the neural network, we would like to use it to predict the labels. You will now implement the \"predict\" function to use the neural network to predict the labels of the training set. This lets you compute the training set accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predict(Theta1, Theta2, X);\n",
    "\n",
    "print('Training Set Accuracy: ', (pred == y-1).mean()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Testing (you can skip this block)\n",
    "To give you an idea of the network's output, you can also run through the examples one at the a time to see what it is predicting. Run the code in the following block to view examples.\n",
    "\n",
    "**NOTE:** to avoid the printing of all the sample instances, you can replace *range(m)* with a small number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Randomly permute examples\n",
    "m = 10\n",
    "rp = np.random.permutation(m)\n",
    "\n",
    "for i in range(m):\n",
    "    print(i)\n",
    "    # Display \n",
    "    print('Displaying Example Image')\n",
    "    tmp = np.transpose(np.expand_dims(X[rp[i], :], axis=1))\n",
    "    displayData(tmp)\n",
    "\n",
    "    pred = predict(Theta1, Theta2, tmp)\n",
    "    print('Neural Network Prediction: ', pred, '(digit ', pred%10, ')')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ================ Part 4: Sigmoid Gradient  ================\n",
    "Before you start implementing backpropagation, you will first implement the gradient for the sigmoid function. You should complete the code in the sigmoidGradient.m file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Evaluating sigmoid gradient...')\n",
    "example = np.array([-15, -1, -0.5, 0, 0.5, 1, 15])\n",
    "example1 = 100000\n",
    "example2 = 0\n",
    "example3 =  np.array([[1, 4, 5], [-5, 8, 9]])\n",
    "g = sigmoidGradient(example3)\n",
    "print('Sigmoid gradient evaluated at', example, ':')\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ================ Part 5: Initializing Pameters ================\n",
    "To learn a two layer neural network that classifies digits. You will start by implementing a function to initialize the weights of the neural network (randInitializeWeights.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Neural Network Parameters ...\n",
      "[ 1.          0.0143566  -0.01333137 ... -0.02920525  0.0547535\n",
      "  0.05331888]\n"
     ]
    }
   ],
   "source": [
    "print('Initializing Neural Network Parameters ...')   \n",
    "\n",
    "initial_Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size)\n",
    "initial_Theta2 = randInitializeWeights(hidden_layer_size, num_labels)\n",
    "\n",
    "# Unroll parameters\n",
    "initial_Theta1 = np.reshape(initial_Theta1, initial_Theta1.size, order='F')\n",
    "initial_Theta2 = np.reshape(initial_Theta2, initial_Theta2.size, order='F')\n",
    "initial_nn_params = np.hstack((initial_Theta1, initial_Theta2))\n",
    "print(initial_nn_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =============== Part 6: Implement Backpropagation ===============\n",
    "Now you will implement the backpropagation algorithm for the neural network. You should add code to nnCostFunction.m to return the partial derivatives of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Backpropagation...\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print('Checking Backpropagation...')\n",
    "\n",
    "#  Check gradients by running checkNNGradients\n",
    "checkNNGradients()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =============== Part 7: Implement Regularization ===============\n",
    "Once your backpropagation implementation is correct, you should now continue to implement the regularization gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Checking Backpropagation (w/ Regularization) ... ')\n",
    "\n",
    "##  Check gradients by running checkNNGradients\n",
    "lambda_value = 3\n",
    "checkNNGradients(lambda_value)\n",
    "\n",
    "# Also output the costFunction debugging values\n",
    "debug_J  = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, \n",
    "                          num_labels, X, y, lambda_value)\n",
    "\n",
    "print('Cost at (fixed) debugging parameters (w/ lambda = 10): ',  debug_J[0][0], \n",
    "      '(this value should be about 0.576051)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =================== Part 8: Training NN ===================\n",
    "You have now implemented all the code necessary to train a neural network. To train your neural network, we will now use \"fmincg\", which is a function which works similarly to \"fminunc\". Recall that these advanced optimizers are able to train our cost functions efficiently as long as we provide them with the gradient computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Neural Network...')\n",
    "\n",
    "#  After you have completed the assignment, change the MaxIter to a larger\n",
    "#  value to see how more training helps.\n",
    "MaxIter = 150\n",
    "\n",
    "#  You should also try different values of lambda\n",
    "lambda_value = 1\n",
    "\n",
    "# Create \"short hand\" for the cost function to be minimized\n",
    "y = np.expand_dims(y, axis=1)\n",
    "\n",
    "costFunction = lambda p : nnCostFunction(p, input_layer_size, hidden_layer_size, \n",
    "                                         num_labels, X, y, lambda_value)\n",
    "\n",
    "# Now, costFunction is a function that takes in only one argument (the\n",
    "# neural network parameters)\n",
    "[nn_params, cost] = fmincg(costFunction, initial_nn_params, MaxIter)\n",
    "\n",
    "# Obtain Theta1 and Theta2 back from nn_params\n",
    "Theta1 = np.reshape(nn_params[0:hidden_layer_size * (input_layer_size + 1)], \n",
    "                              (hidden_layer_size, (input_layer_size + 1)), order='F')\n",
    "Theta2 = np.reshape(nn_params[((hidden_layer_size * (input_layer_size + 1))):],\n",
    "                              (num_labels, (hidden_layer_size + 1)), order='F')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ================= Part 9: Visualize Weights =================\n",
    "You can now \"visualize\" what the neural network is learning by displaying the hidden units to see what features they are capturing in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nVisualizing Neural Network... \\n')\n",
    "\n",
    "displayData(Theta1[:, 1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============= Part 10: Predicting with learned weights =======\n",
    "After training the neural network, we would like to use it to predict the labels. The already implemented \"predict\" function is used by neural network to predict the labels of the training set. This letsyou compute the training set accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predict(Theta1, Theta2, X)\n",
    "pred = np.expand_dims(pred,axis=1)\n",
    "print('Training Set Accuracy: ', (pred == y).mean()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
